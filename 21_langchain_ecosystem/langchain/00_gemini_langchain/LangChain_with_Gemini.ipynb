{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Google Gemini API with LangChain Project**"
      ],
      "metadata": {
        "id": "Sl9hmymDl0kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Environment Setup and Utility Functions**"
      ],
      "metadata": {
        "id": "kU16yqzE84QT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Install Required Package**\n",
        "\n",
        "**LangChain:** A framework for building applications using large language models, facilitating the creation of language model pipelines.\n",
        "\n",
        "**LangChain Google GenAI Integration:** An integration with Google's Generative AI tools, allowing for the use of advanced language models within the LangChain framework."
      ],
      "metadata": {
        "id": "IOBAlT2al7N9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts9Ro5dgQevN",
        "outputId": "b2cc8597-672a-409c-8c23-f4358afbdd18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.8/401.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install the LangChain and LangChain's Google GenAI integration\n",
        "# `-q` keeps the output minimal.\n",
        "# `-U` ensures you are using the latest versions of the packages\n",
        "%pip install -q -U langchain\n",
        "%pip install -q -U langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-api-python-client google-auth google-auth-oauthlib\n"
      ],
      "metadata": {
        "id": "iJJwNWvP1LyK",
        "outputId": "b746ee85-322b-4ae3-d1bd-a253bc20b04b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.137.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib) (1.3.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.24.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.1.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Resetting the Jupyter Notebook Kernel**"
      ],
      "metadata": {
        "id": "cFFl37bP6PDq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OaFyyFGonc3",
        "outputId": "4481315d-84da-485d-fb5e-fa0f3b27b885"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Import the IPython library to access its application instance\n",
        "import IPython\n",
        "\n",
        "# Retrieve the current IPython application instance\n",
        "app = IPython.Application.instance()\n",
        "\n",
        "# Perform a complete shutdown of the current IPython kernel including restarting the kernel\n",
        "# it will help the environment to access the new packages\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Defining Helper Functions**\n",
        "\n",
        "When a model returns simple text, it often lacks the formatting needed to make the content easily digestible. By using markdown formatting, we can enhance the presentation of this plain text, transforming it into a structured and visually appealing format. For instance, converting bullet points into asterisks creates clear lists, while indenting text as blockquotes emphasizes important information. This improved formatting not only enhances readability but also helps users engage more effectively with the content, allowing them to grasp key ideas quickly and easily. Overall, proper formatting is essential for conveying information clearly and making the user experience more enjoyable."
      ],
      "metadata": {
        "id": "yPgxlQ3Z8GGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the textwrap module for text formatting and indentation\n",
        "import textwrap\n",
        "\n",
        "# Import the Markdown display function from IPython to render text as Markdown in Jupyter Notebooks\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Define a function 'to_markdown' that converts a given text into Markdown format\n",
        "def to_markdown(text) -> Markdown:\n",
        "    # Replace bullet points (•) with Markdown-compatible bullet points (*)\n",
        "    text: str = text.replace(\"•\", \"  *\")\n",
        "\n",
        "    # Indent the entire text block with the Markdown blockquote symbol ('> ')\n",
        "    # The lambda function ensures every line is indented\n",
        "    return Markdown(textwrap.indent(text, \"> \", predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "S-OFWytF8Lk_"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Method 01: Using API key for auth**"
      ],
      "metadata": {
        "id": "g6AMbuuLmiiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Get your API key**\n",
        "\n",
        "Before you can use the Gemini API, you must first obtain an API key. If you don't already have one, create a key with one click in Google AI Studio.\n",
        "\n",
        "<a class=\"button button-primary\" href=\"https://makersuite.google.com/app/apikey\" target=\"_blank\" rel=\"noopener noreferrer\">Get an API key</a>"
      ],
      "metadata": {
        "id": "dr_83mNl9rd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Access your API key in colab**"
      ],
      "metadata": {
        "id": "3MUc_0OP-AT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing userdata from Google Colab to securely store and access API keys\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "YBmxOFrh-MpN"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After obtaining the API key, you can access it in Colab\n",
        "\n",
        "* Set the key in the GGEMINI_API_KEY environment variable.\n",
        "* You can save this API key under any variable name you prefer.\n",
        "* Remember to enable access for the saved API key in Colab using the toggle button."
      ],
      "metadata": {
        "id": "rotsgU8f-oYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# after saving api key in env variables\n",
        "# get api key from env\n",
        "google_api_key = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "0NL4LdG4mrj0"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initializing LangChain with GEMINI for AI Chat Responses**"
      ],
      "metadata": {
        "id": "omdHGG_n_Mrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the ChatGoogleGenerativeAI class from the langchain_google_genai module\n",
        "# this will be used for using langchain with gemni\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Import the AIMessage class currently will be used for typing\n",
        "from langchain_core.messages.ai import AIMessage\n",
        "\n",
        "# Initialize an instance of the ChatGoogleGenerativeAI with specific parameters\n",
        "llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",  # Specify the model to use\n",
        "    api_key=google_api_key,     # Provide the Google API key for authentication\n",
        "    temperature=0.2,            # Set the randomness of the model's responses (0 = deterministic, 1 = very random)\n",
        ")"
      ],
      "metadata": {
        "id": "3-PWGEjH_8Q9"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Invoking LangChain Model with Prompt to Get Response**"
      ],
      "metadata": {
        "id": "6nlhwHPDAhF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the LangChain model with a prompt to generate a response\n",
        "ai_msg: AIMessage = llm.invoke(\"What is the capital of France?\")"
      ],
      "metadata": {
        "id": "JCLYKhCynBtr"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display complete response\n",
        "ai_msg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUAhJY2lpn-7",
        "outputId": "0f7e2e37-4e71-4cbe-f304-f44985d367eb"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='It\\'s impossible to definitively say which open-source AI model is \"best\" because the ideal model depends heavily on your specific needs and use case. \\n\\nHere\\'s a breakdown of some popular open-source models and their strengths:\\n\\n**For Language Understanding and Generation:**\\n\\n* **GPT-Neo (EleutherAI):**  A powerful language model with impressive text generation capabilities, comparable to GPT-3 in some aspects. \\n* **BLOOM (BigScience):** A massive multilingual model trained on a vast dataset of text and code, excelling in translation and code generation.\\n* **OPT (Meta AI):**  A large language model focused on efficiency and accessibility, offering a range of sizes for different computational budgets.\\n* **Flan-T5 (Google AI):**  A model specifically designed for instruction following, making it suitable for tasks like question answering and summarization.\\n\\n**For Image Generation:**\\n\\n* **Stable Diffusion (Stability AI):**  A highly versatile model capable of generating realistic and creative images from text prompts.\\n* **DALL-E 2 (OpenAI):**  While not strictly open-source, DALL-E 2\\'s API allows for integration with external applications, making it a powerful tool for image generation.\\n* **Imagen (Google AI):**  Another powerful image generation model, known for its high-quality and realistic outputs.\\n\\n**For Other Tasks:**\\n\\n* **Hugging Face Transformers:**  A library that provides access to a wide range of pre-trained models for various tasks, including text classification, sentiment analysis, and machine translation.\\n* **OpenAI Whisper:**  A powerful speech recognition model capable of transcribing and translating audio.\\n\\n**Factors to Consider When Choosing:**\\n\\n* **Task:** What specific task do you need the model for (e.g., text generation, image generation, translation)?\\n* **Size and Computational Resources:** How much memory and processing power do you have available?\\n* **Language Support:** Does the model support the languages you need?\\n* **License:**  Understand the licensing terms of the model before using it.\\n* **Community Support:**  Look for models with active communities and documentation for easier implementation and troubleshooting.\\n\\n**Recommendation:**\\n\\nInstead of focusing on a single \"best\" model, explore the options based on your specific needs and experiment with different models to find the one that works best for your use case. \\n\\nRemember, the field of open-source AI is constantly evolving, so stay updated on the latest developments and advancements.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-2d811003-787e-4ecb-8e88-392b806df22f-0', usage_metadata={'input_tokens': 16, 'output_tokens': 528, 'total_tokens': 544})"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get actual response\n",
        "ai_msg.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "idD2RwpqpqoU",
        "outputId": "f8756308-e0c9-48dc-c476-4d35af16c684"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'It\\'s impossible to definitively say which open-source AI model is \"best\" because the ideal model depends heavily on your specific needs and use case. \\n\\nHere\\'s a breakdown of some popular open-source models and their strengths:\\n\\n**For Language Understanding and Generation:**\\n\\n* **GPT-Neo (EleutherAI):**  A powerful language model with impressive text generation capabilities, comparable to GPT-3 in some aspects. \\n* **BLOOM (BigScience):** A massive multilingual model trained on a vast dataset of text and code, excelling in translation and code generation.\\n* **OPT (Meta AI):**  A large language model focused on efficiency and accessibility, offering a range of sizes for different computational budgets.\\n* **Flan-T5 (Google AI):**  A model specifically designed for instruction following, making it suitable for tasks like question answering and summarization.\\n\\n**For Image Generation:**\\n\\n* **Stable Diffusion (Stability AI):**  A highly versatile model capable of generating realistic and creative images from text prompts.\\n* **DALL-E 2 (OpenAI):**  While not strictly open-source, DALL-E 2\\'s API allows for integration with external applications, making it a powerful tool for image generation.\\n* **Imagen (Google AI):**  Another powerful image generation model, known for its high-quality and realistic outputs.\\n\\n**For Other Tasks:**\\n\\n* **Hugging Face Transformers:**  A library that provides access to a wide range of pre-trained models for various tasks, including text classification, sentiment analysis, and machine translation.\\n* **OpenAI Whisper:**  A powerful speech recognition model capable of transcribing and translating audio.\\n\\n**Factors to Consider When Choosing:**\\n\\n* **Task:** What specific task do you need the model for (e.g., text generation, image generation, translation)?\\n* **Size and Computational Resources:** How much memory and processing power do you have available?\\n* **Language Support:** Does the model support the languages you need?\\n* **License:**  Understand the licensing terms of the model before using it.\\n* **Community Support:**  Look for models with active communities and documentation for easier implementation and troubleshooting.\\n\\n**Recommendation:**\\n\\nInstead of focusing on a single \"best\" model, explore the options based on your specific needs and experiment with different models to find the one that works best for your use case. \\n\\nRemember, the field of open-source AI is constantly evolving, so stay updated on the latest developments and advancements.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# format the response with markdown\n",
        "to_markdown(ai_msg.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "id": "w4VJvxUZA7Ce",
        "outputId": "481bc132-28a5-454a-e4a5-b18912319832"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> It's impossible to definitively say which open-source AI model is \"best\" because the ideal model depends heavily on your specific needs and use case. \n> \n> Here's a breakdown of some popular open-source models and their strengths:\n> \n> **For Language Understanding and Generation:**\n> \n> * **GPT-Neo (EleutherAI):**  A powerful language model with impressive text generation capabilities, comparable to GPT-3 in some aspects. \n> * **BLOOM (BigScience):** A massive multilingual model trained on a vast dataset of text and code, excelling in translation and code generation.\n> * **OPT (Meta AI):**  A large language model focused on efficiency and accessibility, offering a range of sizes for different computational budgets.\n> * **Flan-T5 (Google AI):**  A model specifically designed for instruction following, making it suitable for tasks like question answering and summarization.\n> \n> **For Image Generation:**\n> \n> * **Stable Diffusion (Stability AI):**  A highly versatile model capable of generating realistic and creative images from text prompts.\n> * **DALL-E 2 (OpenAI):**  While not strictly open-source, DALL-E 2's API allows for integration with external applications, making it a powerful tool for image generation.\n> * **Imagen (Google AI):**  Another powerful image generation model, known for its high-quality and realistic outputs.\n> \n> **For Other Tasks:**\n> \n> * **Hugging Face Transformers:**  A library that provides access to a wide range of pre-trained models for various tasks, including text classification, sentiment analysis, and machine translation.\n> * **OpenAI Whisper:**  A powerful speech recognition model capable of transcribing and translating audio.\n> \n> **Factors to Consider When Choosing:**\n> \n> * **Task:** What specific task do you need the model for (e.g., text generation, image generation, translation)?\n> * **Size and Computational Resources:** How much memory and processing power do you have available?\n> * **Language Support:** Does the model support the languages you need?\n> * **License:**  Understand the licensing terms of the model before using it.\n> * **Community Support:**  Look for models with active communities and documentation for easier implementation and troubleshooting.\n> \n> **Recommendation:**\n> \n> Instead of focusing on a single \"best\" model, explore the options based on your specific needs and experiment with different models to find the one that works best for your use case. \n> \n> Remember, the field of open-source AI is constantly evolving, so stay updated on the latest developments and advancements.\n"
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Method 02: Using JSON file for Auth**"
      ],
      "metadata": {
        "id": "KIB0oYCvpy0l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o97VoIzdXGy"
      },
      "source": [
        "### **Creating a Json file for Auth or allowing Gemini api in Langchain**\n",
        "Adding Api key to environment variables, and getting it's value sometime cause auth issues, In case of facing Auth Issues, Follow the following steps if you are getting Auth related errors in getting response from Gemini, and being asked for auth.\n",
        "  - Open [Google Cloud Console](https://console.cloud.google.com/), at the top left corner, click on **Select a Project**, a new Window screen will pop up, Select an existing one or create a new Project. (Free Version).\n",
        "  - After Selecting or Creating a project, below **WELCOME** screen, Select **APIs and Services** from **Quick Access**.\n",
        "  - On *APIs and Services* windows, look for Library and left Sidebar. Select **Library**.\n",
        "  - In search box, type **Gemini Api**, two results will be shown,\n",
        "      1. *Gemini Api*\n",
        "      2. *Gemini for google cloud*\n",
        "\n",
        "  Select first one. Click on **Enable**. Gemini Api will be enabled.\n",
        "\n",
        "  - Now get back to **APIs and and Services** Window.\n",
        "  - Select **Credentials**, at Top level, after Credentials, Select **Create Credentail**, and click on API Key from the new dropdown. It will generate an api key.\n",
        "  - Click on **Google Cloud**, at top left corner. It will take you to Welcome page.\n",
        "  - From **Quick Access**, Select **IAM and Admin**.\n",
        "  - At left sidebar select **Service accounts** and click on **Create Service Account**\n",
        "      - Fill in the required Details, click on **Create and Continue**.\n",
        "      - Click on **Select Role**, search of *OWNER*, and select the one with full previlages.\n",
        "      - Click on **Continue**.\n",
        "      - Skip the fields, click on **Done**.\n",
        "      - At *Service Accounts* windows, click on the Email, we just created.\n",
        "      - Click the Newly generated Email.\n",
        "      - At the top, below the Key name where it's mentioned, Click on **KEYS**, from the taskbar above,\n",
        "      - At **Keys** window, select **Add Key**.\n",
        "          - Click on **Create new Key**.\n",
        "          - Select **Add Key**.\n",
        "          - Make sure **JSON** is selected.\n",
        "          - Click on **Create Key**. A New dialogue window will pop up telling you that **Private key saved to your computer**\n",
        "          - The .json file is downloaded to your default download folder.\n",
        "Upload it to the root directory of Colab.\n",
        "          - Right click on the newly add file, select **Copy Path**. Replace it with __your path__ in below cell."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setting Up Google Cloud Application Credentials**\n",
        "\n",
        "Configures the environment by setting the GOOGLE_APPLICATION_CREDENTIALS variable. This variable points to the JSON file containing the necessary credentials for authenticating with Google Cloud services. By doing this, the application can securely access Google Cloud resources using the specified credentials."
      ],
      "metadata": {
        "id": "ExG5DkcCCN1j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "gZnwV44lr7MS"
      },
      "outputs": [],
      "source": [
        "# Import the os module to interact with the operating system\n",
        "import os\n",
        "\n",
        "# Set the environment variable for Google Cloud application credentials\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"GEMINI_API_KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initializing LangChain with GEMINI for AI Chat Responses**"
      ],
      "metadata": {
        "id": "k0Bd540kCtmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Invoking LangChain Model with Prompt to Get Response**"
      ],
      "metadata": {
        "id": "v-NA-QtMDAwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the ChatGoogleGenerativeAI class from the langchain_google_genai module\n",
        "# this will be used for using langchain with gemni\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Import the AIMessage class currently will be used for typing\n",
        "from langchain_core.messages.ai import AIMessage\n",
        "\n",
        "# Initialize an instance of the ChatGoogleGenerativeAI with specific parameters\n",
        "# as we are using .json file auth in this case we don't need to specify api key here\n",
        "llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",  # Specify the model to use\n",
        "    temperature=0.2,            # Set the randomness of the model's responses (0 = deterministic, 1 = very random)\n",
        ")"
      ],
      "metadata": {
        "id": "h_opDqtc3zBL",
        "outputId": "12bb5309-155c-4a02-a2b4-c4ef9589e185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DefaultCredentialsError",
          "evalue": "File GEMINI_API_KEY was not found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-29bca6d87baf>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Initialize an instance of the ChatGoogleGenerativeAI with specific parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# as we are using .json file auth in this case we don't need to specify api key here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gemini-1.5-flash\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Specify the model to use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;31m# Set the randomness of the model's responses (0 = deterministic, 1 = very random)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0mgoogle_api_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoogle_api_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0mtransport\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m         self.client = genaix.build_generative_service(\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgoogle_api_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_genai/_genai_extension.py\u001b[0m in \u001b[0;36mbuild_generative_service\u001b[0;34m(credentials, api_key, client_options, client_info, transport)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mclient_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     )\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mv1betaGenerativeServiceClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, credentials, transport, client_options, client_info)\u001b[0m\n\u001b[1;32m    685\u001b[0m             )\n\u001b[1;32m    686\u001b[0m             \u001b[0;31m# initialize with the provided callable or the passed in class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             self._transport = transport_init(\n\u001b[0m\u001b[1;32m    688\u001b[0m                 \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m                 \u001b[0mcredentials_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# The base transport sets the host, credentials and scopes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             )\n\u001b[1;32m     97\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcredentials\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             credentials, _ = google.auth.default(\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mscopes_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquota_project_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquota_project_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/auth/_default.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchecker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m         \u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcredentials\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m             credentials = with_scopes_if_required(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/auth/_default.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# safely set on the returned credentials since requires_scopes will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;31m# guard against setting scopes on user credentials.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_get_explicit_environ_credentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquota_project_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquota_project_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m         \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_get_gcloud_sdk_credentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquota_project_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquota_project_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0m_get_gae_credentials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/auth/_default.py\u001b[0m in \u001b[0;36m_get_explicit_environ_credentials\u001b[0;34m(quota_project_id)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexplicit_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         credentials, project_id = load_credentials_from_file(\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menvironment_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCREDENTIALS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquota_project_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquota_project_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/auth/_default.py\u001b[0m in \u001b[0;36mload_credentials_from_file\u001b[0;34m(filename, scopes, default_scopes, quota_project_id, request)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \"\"\"\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         raise exceptions.DefaultCredentialsError(\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;34m\"File {} was not found.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         )\n",
            "\u001b[0;31mDefaultCredentialsError\u001b[0m: File GEMINI_API_KEY was not found."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the LangChain model with a prompt to generate a response\n",
        "ai_msg : AIMessage = llm.invoke(\"What is the capital of France?\")"
      ],
      "metadata": {
        "id": "zbr5PHvmC_iq"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display complete response\n",
        "ai_msg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TceZg7A6DU7D",
        "outputId": "6a5798d2-2bb3-41a6-b652-7e8691782245"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The capital of France is **Paris**. \\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-d091ffca-0333-4956-9bbb-78e96e069504-0', usage_metadata={'input_tokens': 8, 'output_tokens': 8, 'total_tokens': 16})"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get actual response\n",
        "ai_msg.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KQIvv5RkDVVD",
        "outputId": "4a7657a9-6e21-4c4b-b99e-691a78abad23"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of France is **Paris**. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# format the response with markdown\n",
        "to_markdown(ai_msg.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "id": "_aROZmJfDZri",
        "outputId": "009e5d22-ad94-4264-dff7-1611852d178e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> The capital of France is **Paris**. \n"
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Invoking LangChain Model with Structured Messages for AI Responses**"
      ],
      "metadata": {
        "id": "R7ihQpV-DnTv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "JA4RDvoNkmlk"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List\n",
        "\n",
        "message: List[Dict[str, str]] = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Which open source AI Model is best so far\"}\n",
        "]\n",
        "\n",
        "ai_msg = llm.invoke(message)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ai_msg.content)"
      ],
      "metadata": {
        "id": "Cio28NOyt0KA",
        "outputId": "cf0d0206-c78c-40bc-c339-fb9d2baf75c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It's impossible to definitively say which open-source AI model is \"best\" because the ideal model depends heavily on your specific needs and use case. \n",
            "\n",
            "Here's a breakdown of some popular open-source models and their strengths:\n",
            "\n",
            "**For Language Understanding and Generation:**\n",
            "\n",
            "* **GPT-Neo (EleutherAI):**  A powerful language model with impressive text generation capabilities, comparable to GPT-3 in some aspects. \n",
            "* **BLOOM (BigScience):** A massive multilingual model trained on a vast dataset of text and code, excelling in translation and code generation.\n",
            "* **OPT (Meta AI):**  A large language model focused on efficiency and accessibility, offering a range of sizes for different computational budgets.\n",
            "* **Flan-T5 (Google AI):**  A model specifically designed for instruction following, making it suitable for tasks like question answering and summarization.\n",
            "\n",
            "**For Image Generation:**\n",
            "\n",
            "* **Stable Diffusion (Stability AI):**  A highly versatile model capable of generating realistic and creative images from text prompts.\n",
            "* **DALL-E 2 (OpenAI):**  While not strictly open-source, DALL-E 2's API allows for integration with external applications, making it a powerful tool for image generation.\n",
            "* **Imagen (Google AI):**  Another powerful image generation model, known for its high-quality and realistic outputs.\n",
            "\n",
            "**For Other Tasks:**\n",
            "\n",
            "* **Hugging Face Transformers:**  A library that provides access to a wide range of pre-trained models for various tasks, including text classification, sentiment analysis, and machine translation.\n",
            "* **OpenAI Whisper:**  A powerful speech recognition model capable of transcribing and translating audio.\n",
            "\n",
            "**Factors to Consider When Choosing:**\n",
            "\n",
            "* **Task:** What specific task do you need the model for (e.g., text generation, image generation, translation)?\n",
            "* **Size and Computational Resources:** How much memory and processing power do you have available?\n",
            "* **Language Support:** Does the model support the languages you need?\n",
            "* **License:**  Understand the licensing terms of the model before using it.\n",
            "* **Community Support:**  Look for models with active communities and documentation for easier implementation and troubleshooting.\n",
            "\n",
            "**Recommendation:**\n",
            "\n",
            "Instead of focusing on a single \"best\" model, explore the options based on your specific needs and experiment with different models to find the one that works best for your use case. \n",
            "\n",
            "Remember, the field of open-source AI is constantly evolving, so stay updated on the latest developments and advancements.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(ai_msg.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "id": "d3nc1f0mE943",
        "outputId": "56f45a13-43d4-4c1d-f87b-d398019e1c1c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> It's impossible to definitively say which open-source AI model is \"best\" because the ideal model depends heavily on your specific needs and use case. \n> \n> Here's a breakdown of some popular open-source models and their strengths:\n> \n> **For Language Understanding and Generation:**\n> \n> * **GPT-Neo (EleutherAI):**  A powerful language model with impressive text generation capabilities, comparable to GPT-3 in some aspects. \n> * **BLOOM (BigScience):** A massive multilingual model trained on a vast dataset of text and code, excelling in translation and code generation.\n> * **OPT (Meta AI):**  A large language model focused on efficiency and accessibility, offering a range of sizes for different computational budgets.\n> * **Flan-T5 (Google AI):**  A model specifically designed for instruction following, making it suitable for tasks like question answering and summarization.\n> \n> **For Image Generation:**\n> \n> * **Stable Diffusion (Stability AI):**  A highly versatile model capable of generating realistic and creative images from text prompts.\n> * **DALL-E 2 (OpenAI):**  While not strictly open-source, DALL-E 2's API allows for integration with external applications, making it a powerful tool for image generation.\n> * **Imagen (Google AI):**  Another powerful image generation model, known for its high-quality and realistic outputs.\n> \n> **For Other Tasks:**\n> \n> * **Hugging Face Transformers:**  A library that provides access to a wide range of pre-trained models for various tasks, including text classification, sentiment analysis, and machine translation.\n> * **OpenAI Whisper:**  A powerful speech recognition model capable of transcribing and translating audio.\n> \n> **Factors to Consider When Choosing:**\n> \n> * **Task:** What specific task do you need the model for (e.g., text generation, image generation, translation)?\n> * **Size and Computational Resources:** How much memory and processing power do you have available?\n> * **Language Support:** Does the model support the languages you need?\n> * **License:**  Understand the licensing terms of the model before using it.\n> * **Community Support:**  Look for models with active communities and documentation for easier implementation and troubleshooting.\n> \n> **Recommendation:**\n> \n> Instead of focusing on a single \"best\" model, explore the options based on your specific needs and experiment with different models to find the one that works best for your use case. \n> \n> Remember, the field of open-source AI is constantly evolving, so stay updated on the latest developments and advancements.\n"
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "asRE7om_FbVf"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}