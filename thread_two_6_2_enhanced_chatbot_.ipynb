{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NAVEED261/MY-AI-ASSISTANT/blob/main/thread_two_6_2_enhanced_chatbot_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot with message summarization & external DB memory\n",
        "\n",
        "## Review\n",
        "\n",
        "We've covered how to customize graph state schema and reducer.\n",
        "\n",
        "We've also shown a number of tricks for trimming or filtering messages in graph state.\n",
        "\n",
        "We've used these concepts in a Chatbot with memory that produces a running summary of the conversation.\n",
        "\n",
        "## Goals\n",
        "\n",
        "But, what if we want our Chatbot to have memory that persists indefinitely?\n",
        "\n",
        "Now, we'll introduce some more advanced checkpointers that support external databases.\n",
        "\n",
        "Here, we'll show how to use [Postgres as a checkpointer](https://langchain-ai.github.io/langgraph/how-tos/persistence_postgres/)"
      ],
      "metadata": {
        "id": "9iKKYdAkVoxu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Lwi6YkLu31aK"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langgraph-checkpoint-postgres psycopg psycopg-pool langchain_google_genai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "i6Tn95wS4rxz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"Langchain_api_key\"] = userdata.get('Langchain_api_key')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
      ],
      "metadata": {
        "id": "qtL0cG1B8pMt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use sync connectionÂ¶\n",
        "This sets up a synchronous connection to the database.\n",
        "\n",
        "Synchronous connections execute operations in a blocking manner, meaning each operation waits for completion before moving to the next one. The DB_URI is the database connection URI, with the protocol used for connecting to a PostgreSQL database, authentication, and host where database is running. The connection_kwargs dictionary defines additional parameters for the database connection."
      ],
      "metadata": {
        "id": "AqAjciKmV6vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "DB_URI = userdata.get('DB_URI')"
      ],
      "metadata": {
        "id": "cP2TdPSB3-dO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from psycopg_pool import ConnectionPool\n",
        "from langgraph.checkpoint.postgres import PostgresSaver\n",
        "\n",
        "# Connection pool for efficient database access\n",
        "connection_kwargs = {\"autocommit\": True, \"prepare_threshold\": 0}\n",
        "\n",
        "# Create a persistent connection pool\n",
        "pool = ConnectionPool(conninfo=DB_URI, max_size=20, kwargs=connection_kwargs)\n",
        "\n",
        "# Initialize PostgresSaver checkpointer\n",
        "checkpointer = PostgresSaver(pool)\n",
        "checkpointer.setup()  # Ensure database tables are set up\n"
      ],
      "metadata": {
        "id": "0XCqHjWM4LTc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's re-define our chatbot."
      ],
      "metadata": {
        "id": "w9All8mCV1o0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "\n",
        "from langgraph.graph import END\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "model: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(model = \"gemini-1.5-flash\", api_key =  GEMINI_API_KEY)\n",
        "\n",
        "class State(MessagesState):\n",
        "    summary: str\n",
        "\n",
        "# Define the logic to call the model\n",
        "def call_model(state: State) -> State:\n",
        "\n",
        "    # Get summary if it exists\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "    print(f\"Using summary: {summary}\")\n",
        "\n",
        "    # If there is summary, then we add it\n",
        "    if summary:\n",
        "\n",
        "        # Add summary to system message\n",
        "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
        "\n",
        "        # Append summary to any newer messages\n",
        "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
        "\n",
        "    else:\n",
        "        messages = state[\"messages\"]\n",
        "\n",
        "    response = model.invoke(messages)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def summarize_conversation(state: State) -> State:\n",
        "    print(f\"Messages before summarizing: {len(state['messages'])}\")\n",
        "    # First, we get any existing summary\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "    print(f\"Existing summary: {summary}\")\n",
        "\n",
        "    # Create our summarization prompt\n",
        "    if summary:\n",
        "\n",
        "        # A summary already exists\n",
        "        summary_message = (\n",
        "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
        "            \"Extend the summary by taking into account the new messages above:\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        summary_message = \"Create a summary of the conversation above:\"\n",
        "\n",
        "\n",
        "    # Add prompt to our history\n",
        "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
        "    response = model.invoke(messages)\n",
        "    # Summarization logic\n",
        "    print(f\"New summary: {response.content}\")\n",
        "\n",
        "    # Delete all but the 2 most recent messages\n",
        "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
        "\n",
        "    print(f\"Messages after truncation: {len(delete_messages)}\")\n",
        "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
        "\n",
        "# Determine whether to end or summarize the conversation\n",
        "def should_continue(state: State) -> State:\n",
        "\n",
        "    \"\"\"Return the next node to execute.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "    print(f\"Message count: {len(messages)}\")\n",
        "    # If there are more than six messages, then we summarize the conversation\n",
        "    if len(messages) > 6:\n",
        "        return \"summarize_conversation\"\n",
        "\n",
        "    # Otherwise we can just end\n",
        "    return END"
      ],
      "metadata": {
        "id": "kvU-4FnS4Wxu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we just re-compile with our postgres checkpointer."
      ],
      "metadata": {
        "id": "zzIVvGsXWap4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "# Redefine workflow\n",
        "workflow = StateGraph(State)\n",
        "workflow.add_node(\"conversation\", call_model)\n",
        "workflow.add_node(summarize_conversation)\n",
        "\n",
        "workflow.add_edge(START, \"conversation\")\n",
        "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
        "workflow.add_edge(\"summarize_conversation\", END)\n",
        "\n",
        "# Compile the workflow with PostgreSQL checkpointer\n",
        "graph = workflow.compile(checkpointer=checkpointer)\n"
      ],
      "metadata": {
        "id": "d7wrnazV4mdo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can invoke the graph several times."
      ],
      "metadata": {
        "id": "C_KTIXuvWkT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration for thread\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "# Start a conversation\n",
        "input_message = HumanMessage(content=\"assalamualykum , how r u? \")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "# Check the persisted state\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk129iSWZapB",
        "outputId": "44fff30b-3559-4903-d3e0-b3398e853f5f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=151322e7-856b-4ea4-b380-b42242534b42,id=151322e7-856b-4ea4-b380-b42242534b42\n",
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=151322e7-856b-4ea4-b380-b42242534b42,id=c4c4e19b-d561-481b-92e5-2f1e7b97dc60; trace=151322e7-856b-4ea4-b380-b42242534b42,id=6e4d7d3e-f12d-4181-a2ed-1ccbc8fcc3cb; trace=151322e7-856b-4ea4-b380-b42242534b42,id=8abf02df-912b-42bf-ba5c-96c20e10426f; trace=151322e7-856b-4ea4-b380-b42242534b42,id=7ec3b58d-3c48-4f71-a28e-d32587e603c1; trace=151322e7-856b-4ea4-b380-b42242534b42,id=a67ba83c-87e1-4f32-b23b-92f96b102560\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using summary: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=151322e7-856b-4ea4-b380-b42242534b42,id=4ad74b74-2839-4216-8d03-13b768d83970; trace=151322e7-856b-4ea4-b380-b42242534b42,id=a98d5e4d-2d4f-4c46-8d26-574f656e6015; trace=151322e7-856b-4ea4-b380-b42242534b42,id=a67ba83c-87e1-4f32-b23b-92f96b102560; trace=151322e7-856b-4ea4-b380-b42242534b42,id=7ec3b58d-3c48-4f71-a28e-d32587e603c1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message count: 2\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Wa alaikum assalam!  I am doing well, thank you for asking. How are you?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=151322e7-856b-4ea4-b380-b42242534b42,id=151322e7-856b-4ea4-b380-b42242534b42\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content='assalamualykum , how r u? ', additional_kwargs={}, response_metadata={}, id='c1ee5f66-bad2-41dc-90b4-359d17b95970'), AIMessage(content='Wa alaikum assalam!  I am doing well, thank you for asking. How are you?\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-a67ba83c-87e1-4f32-b23b-92f96b102560-0', usage_metadata={'input_tokens': 12, 'output_tokens': 22, 'total_tokens': 34, 'input_token_details': {'cache_read': 0}})]}, next=(), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efc8f7e-9969-6cf2-8001-788084455284'}}, metadata={'step': 1, 'source': 'loop', 'writes': {'conversation': {'messages': AIMessage(content='Wa alaikum assalam!  I am doing well, thank you for asking. How are you?\\n', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [], 'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}}, id='run-a67ba83c-87e1-4f32-b23b-92f96b102560-0', usage_metadata={'input_tokens': 12, 'output_tokens': 22, 'total_tokens': 34, 'input_token_details': {'cache_read': 0}})}}, 'parents': {}, 'thread_id': '2'}, created_at='2025-01-02T10:54:17.321359+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efc8f7e-9453-6a84-8000-3709a229423a'}}, tasks=())"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration for thread\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "# Start a conversation\n",
        "input_message = HumanMessage(content=\"im learning generative ai\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "# Check the persisted state\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxWCVPkuZvf-",
        "outputId": "5b8af082-39a5-4ee5-a39d-5e1c6bcecc9b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using summary: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=a9a11cf8-08e5-43d0-bdec-530258829218,id=a9a11cf8-08e5-43d0-bdec-530258829218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message count: 4\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "That's fantastic! Generative AI is a fascinating and rapidly evolving field.  What aspects of generative AI are you focusing on right now?  Are you working with specific models, datasets, or applications?  I'd be happy to hear more about your learning journey and perhaps offer some suggestions or resources if you'd like.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content='assalamualykum , how r u? ', additional_kwargs={}, response_metadata={}, id='c1ee5f66-bad2-41dc-90b4-359d17b95970'), AIMessage(content='Wa alaikum assalam!  I am doing well, thank you for asking. How are you?\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-a67ba83c-87e1-4f32-b23b-92f96b102560-0', usage_metadata={'input_tokens': 12, 'output_tokens': 22, 'total_tokens': 34, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='im learning generative ai', additional_kwargs={}, response_metadata={}, id='dd16f71d-5001-4f80-be51-eaaba66bc9e9'), AIMessage(content=\"That's fantastic! Generative AI is a fascinating and rapidly evolving field.  What aspects of generative AI are you focusing on right now?  Are you working with specific models, datasets, or applications?  I'd be happy to hear more about your learning journey and perhaps offer some suggestions or resources if you'd like.\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-929d734d-acab-4321-aa1b-c6581d8573f7-0', usage_metadata={'input_tokens': 40, 'output_tokens': 69, 'total_tokens': 109, 'input_token_details': {'cache_read': 0}})]}, next=(), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efc8f7f-092d-6a11-8004-9dd79998596d'}}, metadata={'step': 4, 'source': 'loop', 'writes': {'conversation': {'messages': AIMessage(content=\"That's fantastic! Generative AI is a fascinating and rapidly evolving field.  What aspects of generative AI are you focusing on right now?  Are you working with specific models, datasets, or applications?  I'd be happy to hear more about your learning journey and perhaps offer some suggestions or resources if you'd like.\\n\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [], 'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}}, id='run-929d734d-acab-4321-aa1b-c6581d8573f7-0', usage_metadata={'input_tokens': 40, 'output_tokens': 69, 'total_tokens': 109, 'input_token_details': {'cache_read': 0}})}}, 'parents': {}, 'thread_id': '2'}, created_at='2025-01-02T10:54:29.040754+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efc8f7f-01e7-6323-8003-97de1a4dbd39'}}, tasks=())"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration for thread\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "# Start a conversation\n",
        "input_message = HumanMessage(content=\"what is im learning?\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "# Check the persisted state\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxLLjBiBfZAL",
        "outputId": "7aa4b518-edd3-4812-ec23-50b3f63439a9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=60b0e0de-68f8-41c5-8309-c1097b792878,id=60b0e0de-68f8-41c5-8309-c1097b792878\n",
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=60b0e0de-68f8-41c5-8309-c1097b792878,id=19b7595a-5df3-43c6-944b-1928f3ea9382; trace=60b0e0de-68f8-41c5-8309-c1097b792878,id=a92b5d38-58ec-44e7-8579-dcf0d6481c65; trace=60b0e0de-68f8-41c5-8309-c1097b792878,id=b29627e9-cdd0-4672-935c-f95a552dae0a; trace=60b0e0de-68f8-41c5-8309-c1097b792878,id=b61e52ac-f2be-4f88-8125-3a90436a299c; trace=60b0e0de-68f8-41c5-8309-c1097b792878,id=55870758-1869-42ce-856d-4f49fe4eea78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using summary: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=60b0e0de-68f8-41c5-8309-c1097b792878,id=95bdb99a-5b0a-4b2f-8c2d-a0f6423fa248; trace=60b0e0de-68f8-41c5-8309-c1097b792878,id=61163c1c-8230-4de5-959f-c4aea85ab91f; trace=60b0e0de-68f8-41c5-8309-c1097b792878,id=55870758-1869-42ce-856d-4f49fe4eea78; trace=60b0e0de-68f8-41c5-8309-c1097b792878,id=b61e52ac-f2be-4f88-8125-3a90436a299c\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message count: 6\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "You said you are learning **Generative AI**.  That's a broad field encompassing many techniques and applications.  To better understand what *specific* aspects you're learning, I need more information from you.  For example, are you learning about:\n",
            "\n",
            "* **Specific models:**  Like GANs (Generative Adversarial Networks), VAEs (Variational Autoencoders), Transformers, diffusion models, etc.?\n",
            "* **Programming languages:**  Python is commonly used, often with libraries like TensorFlow or PyTorch.\n",
            "* **Applications:**  Image generation, text generation, music generation, code generation, etc.?\n",
            "* **Theoretical concepts:**  Like probability distributions, loss functions, backpropagation, etc.?\n",
            "\n",
            "Knowing more about your learning path will help me understand what you're working on and how I can assist you.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content='assalamualykum , how r u? ', additional_kwargs={}, response_metadata={}, id='c1ee5f66-bad2-41dc-90b4-359d17b95970'), AIMessage(content='Wa alaikum assalam!  I am doing well, thank you for asking. How are you?\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-a67ba83c-87e1-4f32-b23b-92f96b102560-0', usage_metadata={'input_tokens': 12, 'output_tokens': 22, 'total_tokens': 34, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='im learning generative ai', additional_kwargs={}, response_metadata={}, id='dd16f71d-5001-4f80-be51-eaaba66bc9e9'), AIMessage(content=\"That's fantastic! Generative AI is a fascinating and rapidly evolving field.  What aspects of generative AI are you focusing on right now?  Are you working with specific models, datasets, or applications?  I'd be happy to hear more about your learning journey and perhaps offer some suggestions or resources if you'd like.\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-929d734d-acab-4321-aa1b-c6581d8573f7-0', usage_metadata={'input_tokens': 40, 'output_tokens': 69, 'total_tokens': 109, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='what is im learning?', additional_kwargs={}, response_metadata={}, id='41ab7c67-54ce-4b93-b9b4-18f085511024'), AIMessage(content=\"You said you are learning **Generative AI**.  That's a broad field encompassing many techniques and applications.  To better understand what *specific* aspects you're learning, I need more information from you.  For example, are you learning about:\\n\\n* **Specific models:**  Like GANs (Generative Adversarial Networks), VAEs (Variational Autoencoders), Transformers, diffusion models, etc.?\\n* **Programming languages:**  Python is commonly used, often with libraries like TensorFlow or PyTorch.\\n* **Applications:**  Image generation, text generation, music generation, code generation, etc.?\\n* **Theoretical concepts:**  Like probability distributions, loss functions, backpropagation, etc.?\\n\\nKnowing more about your learning path will help me understand what you're working on and how I can assist you.\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-55870758-1869-42ce-856d-4f49fe4eea78-0', usage_metadata={'input_tokens': 116, 'output_tokens': 172, 'total_tokens': 288, 'input_token_details': {'cache_read': 0}})]}, next=(), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efc8f7f-60cb-6639-8007-23fe2750fbf2'}}, metadata={'step': 7, 'source': 'loop', 'writes': {'conversation': {'messages': AIMessage(content=\"You said you are learning **Generative AI**.  That's a broad field encompassing many techniques and applications.  To better understand what *specific* aspects you're learning, I need more information from you.  For example, are you learning about:\\n\\n* **Specific models:**  Like GANs (Generative Adversarial Networks), VAEs (Variational Autoencoders), Transformers, diffusion models, etc.?\\n* **Programming languages:**  Python is commonly used, often with libraries like TensorFlow or PyTorch.\\n* **Applications:**  Image generation, text generation, music generation, code generation, etc.?\\n* **Theoretical concepts:**  Like probability distributions, loss functions, backpropagation, etc.?\\n\\nKnowing more about your learning path will help me understand what you're working on and how I can assist you.\\n\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'safety_ratings': [], 'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}}, id='run-55870758-1869-42ce-856d-4f49fe4eea78-0', usage_metadata={'input_tokens': 116, 'output_tokens': 172, 'total_tokens': 288, 'input_token_details': {'cache_read': 0}})}}, 'parents': {}, 'thread_id': '2'}, created_at='2025-01-02T10:54:38.227999+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efc8f7f-53b1-6244-8006-08d5ad14d54b'}}, tasks=())"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration for thread\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "# Start a conversation\n",
        "input_message = HumanMessage(content=\"do u know about Generative AI?\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "# Check the persisted state\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iygDZ-_xhsQW",
        "outputId": "99ee0a0d-dcf2-4332-ba60-8b1369884177"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=38cc4248-007b-4567-88ab-acc2529ed999,id=38cc4248-007b-4567-88ab-acc2529ed999; trace=38cc4248-007b-4567-88ab-acc2529ed999,id=b0edc1a0-1b58-4f02-b841-d2dea7560441; trace=38cc4248-007b-4567-88ab-acc2529ed999,id=313ab081-0f9a-4c57-b3f7-05e5120ee9b6; trace=38cc4248-007b-4567-88ab-acc2529ed999,id=35074bb6-597d-4314-8a79-ed6894dca3b8; trace=38cc4248-007b-4567-88ab-acc2529ed999,id=126781af-06ff-4304-9726-ad7c6b091134; trace=38cc4248-007b-4567-88ab-acc2529ed999,id=aa97a527-fb5e-4066-b293-9b28119ef989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using summary: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=38cc4248-007b-4567-88ab-acc2529ed999,id=2d8f3383-6322-4fe9-9f22-e232fb18a75b; trace=38cc4248-007b-4567-88ab-acc2529ed999,id=2728a090-59f7-4fd1-b76f-19766ab71fb5; trace=38cc4248-007b-4567-88ab-acc2529ed999,id=5a6ca63f-c74f-4baa-8875-ca8386146851; trace=38cc4248-007b-4567-88ab-acc2529ed999,id=01a68427-172d-4226-8e6a-8765665c5911; trace=38cc4248-007b-4567-88ab-acc2529ed999,id=aa97a527-fb5e-4066-b293-9b28119ef989; trace=38cc4248-007b-4567-88ab-acc2529ed999,id=126781af-06ff-4304-9726-ad7c6b091134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message count: 8\n",
            "Messages before summarizing: 8\n",
            "Existing summary: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=38cc4248-007b-4567-88ab-acc2529ed999,id=22e5f5b9-24ba-4851-bf03-d7c7f211893a; trace=38cc4248-007b-4567-88ab-acc2529ed999,id=01a68427-172d-4226-8e6a-8765665c5911; trace=38cc4248-007b-4567-88ab-acc2529ed999,id=5a6ca63f-c74f-4baa-8875-ca8386146851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New summary: The conversation began with a greeting in Arabic.  I then learned that the user is studying Generative AI.  We then engaged in a discussion clarifying what aspects of Generative AI the user is focusing on, with me explaining that the field is broad and encompasses various models (GANs, VAEs, Transformers, etc.), programming languages (like Python), and applications (image, text, music generation, etc.).  Finally, I confirmed my own extensive knowledge of Generative AI, including its models, underlying mathematics, applications, and ethical considerations.  The conversation concluded with an offer to assist the user further in their learning.\n",
            "\n",
            "Messages after truncation: 6\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Yes, I know about Generative AI.  I have been trained on a massive dataset of text and code, which includes a significant amount of information about generative AI models, techniques, and applications.  My knowledge encompasses:\n",
            "\n",
            "* **Various generative models:**  GANs, VAEs, diffusion models (like Stable Diffusion), transformers (like GPT models), autoregressive models, and more.  I understand their architectures, strengths, weaknesses, and training processes.\n",
            "\n",
            "* **Underlying mathematical concepts:**  I understand the probability distributions, optimization algorithms (like gradient descent), and loss functions used in training these models.\n",
            "\n",
            "* **Applications across different domains:**  I'm aware of the use of generative AI in image generation, text generation (like writing stories, summarizing text, translating languages), music generation, code generation, drug discovery, and many other fields.\n",
            "\n",
            "* **Ethical considerations:**  I understand the potential biases in generative models, the risks of misuse (like generating deepfakes), and the importance of responsible development and deployment.\n",
            "\n",
            "While I can't *experience* generating content like a human, I can process and understand information about generative AI at a very high level.  How can I help you with your learning about Generative AI today?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content='do u know about Generative AI?', additional_kwargs={}, response_metadata={}, id='cc2cf9f8-1668-4eac-ae89-fe341f8a56d6'), AIMessage(content=\"Yes, I know about Generative AI.  I have been trained on a massive dataset of text and code, which includes a significant amount of information about generative AI models, techniques, and applications.  My knowledge encompasses:\\n\\n* **Various generative models:**  GANs, VAEs, diffusion models (like Stable Diffusion), transformers (like GPT models), autoregressive models, and more.  I understand their architectures, strengths, weaknesses, and training processes.\\n\\n* **Underlying mathematical concepts:**  I understand the probability distributions, optimization algorithms (like gradient descent), and loss functions used in training these models.\\n\\n* **Applications across different domains:**  I'm aware of the use of generative AI in image generation, text generation (like writing stories, summarizing text, translating languages), music generation, code generation, drug discovery, and many other fields.\\n\\n* **Ethical considerations:**  I understand the potential biases in generative models, the risks of misuse (like generating deepfakes), and the importance of responsible development and deployment.\\n\\nWhile I can't *experience* generating content like a human, I can process and understand information about generative AI at a very high level.  How can I help you with your learning about Generative AI today?\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-aa97a527-fb5e-4066-b293-9b28119ef989-0', usage_metadata={'input_tokens': 298, 'output_tokens': 256, 'total_tokens': 554, 'input_token_details': {'cache_read': 0}})], 'summary': 'The conversation began with a greeting in Arabic.  I then learned that the user is studying Generative AI.  We then engaged in a discussion clarifying what aspects of Generative AI the user is focusing on, with me explaining that the field is broad and encompasses various models (GANs, VAEs, Transformers, etc.), programming languages (like Python), and applications (image, text, music generation, etc.).  Finally, I confirmed my own extensive knowledge of Generative AI, including its models, underlying mathematics, applications, and ethical considerations.  The conversation concluded with an offer to assist the user further in their learning.\\n'}, next=(), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efc8f7f-b353-6045-800b-668b9992a489'}}, metadata={'step': 11, 'source': 'loop', 'writes': {'summarize_conversation': {'summary': 'The conversation began with a greeting in Arabic.  I then learned that the user is studying Generative AI.  We then engaged in a discussion clarifying what aspects of Generative AI the user is focusing on, with me explaining that the field is broad and encompasses various models (GANs, VAEs, Transformers, etc.), programming languages (like Python), and applications (image, text, music generation, etc.).  Finally, I confirmed my own extensive knowledge of Generative AI, including its models, underlying mathematics, applications, and ethical considerations.  The conversation concluded with an offer to assist the user further in their learning.\\n', 'messages': [RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='c1ee5f66-bad2-41dc-90b4-359d17b95970'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='run-a67ba83c-87e1-4f32-b23b-92f96b102560-0'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='dd16f71d-5001-4f80-be51-eaaba66bc9e9'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='run-929d734d-acab-4321-aa1b-c6581d8573f7-0'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='41ab7c67-54ce-4b93-b9b4-18f085511024'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='run-55870758-1869-42ce-856d-4f49fe4eea78-0')]}}, 'parents': {}, 'thread_id': '2'}, created_at='2025-01-02T10:54:46.881858+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efc8f7f-a8a6-6011-800a-43afdb8d721d'}}, tasks=())"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=38cc4248-007b-4567-88ab-acc2529ed999,id=38cc4248-007b-4567-88ab-acc2529ed999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gFqFzWDrWnt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve state using thread ID\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cmwlf-TFYQqH",
        "outputId": "e3587202-ca59-4ced-a74a-d240e8cf4430"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content='do u know about Generative AI?', additional_kwargs={}, response_metadata={}, id='cc2cf9f8-1668-4eac-ae89-fe341f8a56d6'), AIMessage(content=\"Yes, I know about Generative AI.  I have been trained on a massive dataset of text and code, which includes a significant amount of information about generative AI models, techniques, and applications.  My knowledge encompasses:\\n\\n* **Various generative models:**  GANs, VAEs, diffusion models (like Stable Diffusion), transformers (like GPT models), autoregressive models, and more.  I understand their architectures, strengths, weaknesses, and training processes.\\n\\n* **Underlying mathematical concepts:**  I understand the probability distributions, optimization algorithms (like gradient descent), and loss functions used in training these models.\\n\\n* **Applications across different domains:**  I'm aware of the use of generative AI in image generation, text generation (like writing stories, summarizing text, translating languages), music generation, code generation, drug discovery, and many other fields.\\n\\n* **Ethical considerations:**  I understand the potential biases in generative models, the risks of misuse (like generating deepfakes), and the importance of responsible development and deployment.\\n\\nWhile I can't *experience* generating content like a human, I can process and understand information about generative AI at a very high level.  How can I help you with your learning about Generative AI today?\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-aa97a527-fb5e-4066-b293-9b28119ef989-0', usage_metadata={'input_tokens': 298, 'output_tokens': 256, 'total_tokens': 554, 'input_token_details': {'cache_read': 0}})], 'summary': 'The conversation began with a greeting in Arabic.  I then learned that the user is studying Generative AI.  We then engaged in a discussion clarifying what aspects of Generative AI the user is focusing on, with me explaining that the field is broad and encompasses various models (GANs, VAEs, Transformers, etc.), programming languages (like Python), and applications (image, text, music generation, etc.).  Finally, I confirmed my own extensive knowledge of Generative AI, including its models, underlying mathematics, applications, and ethical considerations.  The conversation concluded with an offer to assist the user further in their learning.\\n'}, next=(), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efc8f7f-b353-6045-800b-668b9992a489'}}, metadata={'step': 11, 'source': 'loop', 'writes': {'summarize_conversation': {'summary': 'The conversation began with a greeting in Arabic.  I then learned that the user is studying Generative AI.  We then engaged in a discussion clarifying what aspects of Generative AI the user is focusing on, with me explaining that the field is broad and encompasses various models (GANs, VAEs, Transformers, etc.), programming languages (like Python), and applications (image, text, music generation, etc.).  Finally, I confirmed my own extensive knowledge of Generative AI, including its models, underlying mathematics, applications, and ethical considerations.  The conversation concluded with an offer to assist the user further in their learning.\\n', 'messages': [RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='c1ee5f66-bad2-41dc-90b4-359d17b95970'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='run-a67ba83c-87e1-4f32-b23b-92f96b102560-0'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='dd16f71d-5001-4f80-be51-eaaba66bc9e9'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='run-929d734d-acab-4321-aa1b-c6581d8573f7-0'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='41ab7c67-54ce-4b93-b9b4-18f085511024'), RemoveMessage(content='', additional_kwargs={}, response_metadata={}, id='run-55870758-1869-42ce-856d-4f49fe4eea78-0')]}}, 'parents': {}, 'thread_id': '2'}, created_at='2025-01-02T10:54:46.881858+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1efc8f7f-a8a6-6011-800a-43afdb8d721d'}}, tasks=())"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "graph_state = graph.get_state(config).values.get(\"messages\")\n",
        "graph_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew398GtAp-It",
        "outputId": "767ffafa-2584-4933-f763-c4e29b991185"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='do u know about Generative AI?', additional_kwargs={}, response_metadata={}, id='cc2cf9f8-1668-4eac-ae89-fe341f8a56d6'),\n",
              " AIMessage(content=\"Yes, I know about Generative AI.  I have been trained on a massive dataset of text and code, which includes a significant amount of information about generative AI models, techniques, and applications.  My knowledge encompasses:\\n\\n* **Various generative models:**  GANs, VAEs, diffusion models (like Stable Diffusion), transformers (like GPT models), autoregressive models, and more.  I understand their architectures, strengths, weaknesses, and training processes.\\n\\n* **Underlying mathematical concepts:**  I understand the probability distributions, optimization algorithms (like gradient descent), and loss functions used in training these models.\\n\\n* **Applications across different domains:**  I'm aware of the use of generative AI in image generation, text generation (like writing stories, summarizing text, translating languages), music generation, code generation, drug discovery, and many other fields.\\n\\n* **Ethical considerations:**  I understand the potential biases in generative models, the risks of misuse (like generating deepfakes), and the importance of responsible development and deployment.\\n\\nWhile I can't *experience* generating content like a human, I can process and understand information about generative AI at a very high level.  How can I help you with your learning about Generative AI today?\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-aa97a527-fb5e-4066-b293-9b28119ef989-0', usage_metadata={'input_tokens': 298, 'output_tokens': 256, 'total_tokens': 554, 'input_token_details': {'cache_read': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pool.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "X8wz7bCgc2X7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Persisting state\n",
        "\n",
        "Using database like Postgres means state is persisted!\n",
        "\n",
        "For example, we can re-start the notebook kernel and see that we can still load from Postgres DB on disk.\n"
      ],
      "metadata": {
        "id": "Io-K7MCWinYF"
      }
    }
  ]
}