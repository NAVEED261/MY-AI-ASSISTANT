{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NAVEED261/MY-AI-ASSISTANT/blob/main/2_2_chains_reducers_learn_applied_generative_ai_fundamentals_03_langchain_ecosystem_langgraph_course_notebooks_module_1_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cbf2458",
      "metadata": {
        "id": "4cbf2458"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-1/chain.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58238466-lesson-4-chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee55d3da-c53a-4c76-b46f-8e0d602e072e",
      "metadata": {
        "id": "ee55d3da-c53a-4c76-b46f-8e0d602e072e"
      },
      "source": [
        "# Chain\n",
        "\n",
        "# Review\n",
        "Hum ne aik sada graph banaya hai jiss mein nodes, normal edges, aur conditional edges shamil hain.\n",
        "\n",
        "# Goals\n",
        "Ab chaliye aik sada chain tak pohanchte hain jiss mein 4 concepts ko milaya jaye:\n",
        "\n",
        "Chat messages ko apni graph state ke taur par istemal karna.\n",
        "Graph nodes mein chat models ko istemal karna.\n",
        "Tools ko chat model se baandhna (bind karna).\n",
        "Graph nodes mein tool calls ko execute karna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a55e2e80-a718-4aaf-99b9-371157b34a4b",
      "metadata": {
        "id": "a55e2e80-a718-4aaf-99b9-371157b34a4b"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain-google-genai langchain_core langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae5ac2d0-c7b0-4a20-86e5-4b6ed15ec20e",
      "metadata": {
        "id": "ae5ac2d0-c7b0-4a20-86e5-4b6ed15ec20e"
      },
      "source": [
        "## Messages\n",
        "\n",
        "Chat models messages ko istemal kar sakte hain, jiss se conversation mein mukhtalif roles capture ho jate hain.\n",
        "\n",
        "LangChain mukhtalif message types ko support karta hai, jin mein shamil hain: HumanMessage, AIMessage, SystemMessage, aur ToolMessage.\n",
        "\n",
        "Yeh sab kuch represent karte hain:\n",
        "\n",
        "HumanMessage: User ki taraf se bheja gaya message.\n",
        "AIMessage: Chat model ki taraf se bheja gaya message.\n",
        "SystemMessage: Chat model ko kisi specific behavior ke liye instructions dene wala message.\n",
        "ToolMessage: Tool call se aane wala message.\n",
        "Chaliye ab messages ki aik list banaate hain.\n",
        "\n",
        "Har message mein kuch cheezein shamil ho sakti hain:\n",
        "\n",
        "content - message ka content (matn).\n",
        "name - optional, message likhne wale ka naam.\n",
        "response_metadata - optional, aik metadata ka dict (misal ke taur par, model provider ki taraf se AIMessage ke liye diya gaya metadata).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "866b5321-a238-4a9e-af9e-f11a131b5f11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "866b5321-a238-4a9e-af9e-f11a131b5f11",
        "outputId": "b336a558-be8e-4921-fcfc-99ed6a5171a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: Model\n",
            "\n",
            "So you said you were researching ocean mammals?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "Name: Lance\n",
            "\n",
            "Yes, that's right.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Name: Model\n",
            "\n",
            "Great, what would you like to learn about.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "Name: Lance\n",
            "\n",
            "I want to learn about the best place to see Orcas in the US.\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "messages = [AIMessage(content=f\"So you said you were researching ocean mammals?\", name=\"Model\")]\n",
        "messages.append(HumanMessage(content=f\"Yes, that's right.\",name=\"Lance\"))\n",
        "messages.append(AIMessage(content=f\"Great, what would you like to learn about.\", name=\"Model\"))\n",
        "messages.append(HumanMessage(content=f\"I want to learn about the best place to see Orcas in the US.\", name=\"Lance\"))\n",
        "# yha model ka mtlb llm ha inshort chatgpt , gemnai etc etc\n",
        "for m in messages:\n",
        "    m.pretty_print()\n",
        "    # jub b hm conversation krta ha to llm k pas is trh sa conversation jati ha"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ca48df0-b639-4ff1-a777-ffe2185d991e",
      "metadata": {
        "id": "0ca48df0-b639-4ff1-a777-ffe2185d991e"
      },
      "source": [
        "## Chat Models\n",
        "\n",
        "Chat models messages ki aik sequence ko input ke taur par istemal kar sakte hain aur woh message types ko support karte hain, jaisa ke pehle discuss kiya gaya hai.\n",
        "\n",
        "Bohat saare options hain chunne ke liye! Chaliye Gemini ke saath kaam karte hain.\n",
        "\n",
        "Pehle yeh check kar lete hain ke aapka GEMINI_API_KEY set hai ya nahi, aur agar set nahi hai, toh aap se yeh enter karne ko kaha jaye ga."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2652d5ec-7602-4220-bc6e-b90783ab287b",
      "metadata": {
        "id": "2652d5ec-7602-4220-bc6e-b90783ab287b"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "google_api_key = userdata.get('my_stenographer_key')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceae53d4-14f5-4bf3-a953-cc465240f5b5",
      "metadata": {
        "id": "ceae53d4-14f5-4bf3-a953-cc465240f5b5"
      },
      "source": [
        "Hum aik chat model load kar sakte hain aur apni messages ki list ke saath isko invoke kar sakte hain.\n",
        "\n",
        "Hum dekh sakte hain ke jo result milega woh aik AIMessage hoga, jiss mein specific response_metadata shamil hogi.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39747099",
      "metadata": {
        "id": "39747099"
      },
      "source": [
        "**Note:** Zaroori json key Google Cloud Console se hasil karein, instructions ko follow karte hue jo step 21_langchain_ecosystem/langchain/-01_gemini_standalone/Gemini_API_python.ipynb file mein di gayi hain. Jaisay hi yeh key hasil ho jaye, isko Google Colab mein load karein taa ke project mein aagay barh sakein."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95b99ad4-5753-49d3-a916-a9e949722c01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95b99ad4-5753-49d3-a916-a9e949722c01",
        "outputId": "e83dc8fa-e3ff-4641-9c3c-bca87312d283"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"That's a great question!  While Orcas are found in both the Atlantic and Pacific Oceans, the best places to see them in the US are definitely in the **Pacific Northwest** and **Alaska**. \\n\\nHere's a breakdown:\\n\\n**Pacific Northwest:**\\n\\n* **San Juan Islands, Washington:**  This is one of the most popular and reliable places to see Orcas.  The waters around the islands are home to a resident population of Orcas that feed on salmon.  You can go on whale watching tours from various towns like Friday Harbor, Anacortes, and Seattle.\\n* **Olympic Peninsula, Washington:** This area also has a good chance of spotting Orcas, especially near the Strait of Juan de Fuca.\\n* **Oregon Coast:** While Orcas are less frequent here, you can still have a chance to see them, particularly in the fall when they follow salmon runs.\\n\\n**Alaska:**\\n\\n* **Southeast Alaska:** This region is a prime spot for Orcas. You can see them in the waters around Juneau, Ketchikan, and Sitka. \\n* **Prince William Sound:** This area is known for its abundant marine life, including Orcas.\\n* **Kenai Fjords National Park:** Orcas can be seen in the fjords and surrounding waters.\\n\\n**Important Considerations:**\\n\\n* **Seasonality:** The best time to see Orcas in the Pacific Northwest is during the summer months (June-September) when salmon runs are abundant.  In Alaska, you have a good chance of seeing Orcas year-round.\\n* **Whale Watching Tours:**  It's highly recommended to go on a whale watching tour with a reputable company. They have experienced guides who can help you spot Orcas and provide information about their behavior.\\n* **Respect:** Remember that Orcas are wild animals.  It's important to maintain a safe distance and avoid disturbing their natural behavior.\\n\\nI hope this helps!  Do you have any other questions about Orcas or whale watching? \\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-6c01bc26-e796-4477-8faf-a4af3928a366-0', usage_metadata={'input_tokens': 46, 'output_tokens': 416, 'total_tokens': 462, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", api_key=google_api_key)\n",
        "result = llm.invoke(messages)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD_Sj9XYARGx",
        "outputId": "c1440417-5908-4a3d-94dd-090670c30f3b"
      },
      "id": "KD_Sj9XYARGx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.messages.ai.AIMessage'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3a29654-6b8e-4eda-9cec-22fabb9b8620",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3a29654-6b8e-4eda-9cec-22fabb9b8620",
        "outputId": "e040cae6-9560-4213-e887-934ee2d144fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},\n",
              " 'finish_reason': 'STOP',\n",
              " 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT',\n",
              "   'probability': 'NEGLIGIBLE',\n",
              "   'blocked': False},\n",
              "  {'category': 'HARM_CATEGORY_HATE_SPEECH',\n",
              "   'probability': 'NEGLIGIBLE',\n",
              "   'blocked': False},\n",
              "  {'category': 'HARM_CATEGORY_HARASSMENT',\n",
              "   'probability': 'NEGLIGIBLE',\n",
              "   'blocked': False},\n",
              "  {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT',\n",
              "   'probability': 'NEGLIGIBLE',\n",
              "   'blocked': False}]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "result.response_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4718bd5c-5314-4405-a164-f1fe912ae306",
      "metadata": {
        "id": "4718bd5c-5314-4405-a164-f1fe912ae306"
      },
      "source": [
        "## Tools\n",
        "\n",
        "Tools tab faidemand hote hain jab aap chahte hain ke model external systems se interact kare.\n",
        "\n",
        "External systems (misal ke taur par, APIs) aam taur par ek khas input schema ya payload mangte hain, na ke natural language.\n",
        "\n",
        "Jab hum ek API ko, misal ke taur par, tool ke taur par bind karte hain, toh hum model ko required input schema ka awareness dete hain.\n",
        "\n",
        "Model user ke natural language input ke basis par decide karega ke kab kisi tool ko call karna hai.\n",
        "\n",
        "Aur phir yeh output wapas karega jo ke tool ke schema ke mutabiq hoga.\n",
        "\n",
        "Bohat se LLM providers tool calling ko support karte hain aur LangChain mein tool calling interface bohat simple hai.\n",
        "\n",
        "Aap aasani se kisi bhi Python function ko pass kar sakte hain ChatModel.bind_tools(function) mein."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17a942b1",
      "metadata": {
        "id": "17a942b1"
      },
      "source": [
        "Let's showcase a simple example of tool calling!\n",
        "\n",
        "The `multiply` function is our tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "928faf56-1a1a-4c5f-b97d-bd64d8e166d1",
      "metadata": {
        "id": "928faf56-1a1a-4c5f-b97d-bd64d8e166d1"
      },
      "outputs": [],
      "source": [
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "llm_with_tools: ChatGoogleGenerativeAI = llm.bind_tools([multiply])\n",
        "# llm upna bind_tool ko use kra or llm_with_tools jiski type ChatGoogleGenerativeAI\n",
        "# ha sa integrate ho or multiply function ka response genertre kro."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(multiply(5,6))\n",
        "# just chekc because this answer will generate by llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UgwVGn1CIFZ",
        "outputId": "8ea2a579-4736-4756-a9b2-70d51d5aaf4c"
      },
      "id": "_UgwVGn1CIFZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a3f9dba",
      "metadata": {
        "id": "8a3f9dba"
      },
      "source": [
        "Warning yeh indicate kar rahi hai ke title key andar hi generate ho rahi hai, mumkin hai ke yeh LangGraph ya Gemini integration ke andar ho jab tool bind ho raha hai, bajaye iske ke hum isko explicitly pass karein. Isliye is warning ko nazarandaz karein.\n",
        "\n",
        "Agar hum ek input dein - misal ke taur par, \"2 ko 3 se multiply karne ka jawab kya hai\" - toh humein ek function_call wapas milta hai.\n",
        "\n",
        "Yeh function_call ke andar specific arguments hain jo humare function ke input schema se match karte hain, saath hi function ka naam bhi jiska call karna hai.\n",
        "\n",
        "\n",
        "{'name': 'multiply', 'arguments': '{\"b\": 3.0, \"a\": 2.0}'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9edbe13e-cc72-4685-ac97-2ebb4ceb2544",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9edbe13e-cc72-4685-ac97-2ebb4ceb2544",
        "outputId": "7d113c7e-353f-4ceb-8b73-0ad37abdec48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"b\": 3.0, \"a\": 2.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-c9eecf6d-d792-497f-9d77-4de8e10bb09b-0', tool_calls=[{'name': 'multiply', 'args': {'b': 3.0, 'a': 2.0}, 'id': '47dfc6c6-6b52-4cb5-9f49-b110a4f2870d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 18, 'total_tokens': 75, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "llm_with_tools.invoke([HumanMessage(content=f\"What is 2 multiplied by 3\", name=\"Lance\")])\n",
        "# is zrea hama pora funnction or tool ki dono id b mili ha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a78178cb-fa43-45b5-be5e-5a22bda5a5e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a78178cb-fa43-45b5-be5e-5a22bda5a5e7",
        "outputId": "d9b86bae-9e06-40e7-9b5a-47460a08876f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'multiply', 'arguments': '{\"b\": 3.0, \"a\": 2.0}'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "function_call.additional_kwargs['function_call']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools.invoke([HumanMessage(content=f\"What is 2 multiplied by 6\", name=\"Lance\")])\n",
        "# again  islya check kya ha k dono id khi same to ni ki c na duplicate to ni banna li"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXQOFI34IJGi",
        "outputId": "a0c3ab88-63bd-48e1-918b-907fbc71317f"
      },
      "id": "jXQOFI34IJGi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"b\": 6.0, \"a\": 2.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-4cb84246-c512-485e-82de-5514f603a8d2-0', tool_calls=[{'name': 'multiply', 'args': {'b': 6.0, 'a': 2.0}, 'id': '20ed82d1-ef63-4ebf-96ea-32b14f2bdaf8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 57, 'output_tokens': 18, 'total_tokens': 75, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21c10f9a-2372-486b-9305-55b7c41ecd6e",
      "metadata": {
        "id": "21c10f9a-2372-486b-9305-55b7c41ecd6e"
      },
      "source": [
        "## Using messages as state\n",
        "\n",
        "in bunyaadon ko tayyar karne ke baad, ab hum apni graph state mein messages istemal kar sakte hain.\n",
        "\n",
        "Chaliye apni state ko define karte hain, MessagesState, jisko hum aik TypedDict ke taur par define karenge jiss mein aik hi key hogi: messages.\n",
        "\n",
        "messages aik list hai messages ki, jaisa ke hum ne upar define kiya tha (misal ke taur par, HumanMessage, waghera)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3699dd5c-398c-43c7-b496-fd87e55e11ca",
      "metadata": {
        "id": "3699dd5c-398c-43c7-b496-fd87e55e11ca"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "from langchain_core.messages import AnyMessage\n",
        "\n",
        "class MessagesState(TypedDict):\n",
        "    messages: list[AnyMessage]\n",
        "    # yha only list ki type btai ha jisma koi b msg hoskta ha like as IA, HUMAN, TOOL, SYSTEM MSG.but ya over right hgoa\n",
        "    #  means jub list ma \"hi\"karaga to  \"by\" likhahoga inshort overright hojaiga"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "211cba3e-ebba-4b91-a539-1cbc28b4a40e",
      "metadata": {
        "id": "211cba3e-ebba-4b91-a539-1cbc28b4a40e"
      },
      "source": [
        "## Reducers\n",
        "\n",
        "Ab humare paas aik chhoti si problem hai!\n",
        "\n",
        "Jaisa ke hum ne discuss kiya tha, har node humare state key messages ke liye aik naya value return karega.\n",
        "\n",
        "Lekin, yeh naya value pichle messages value ko overwrite kar dega.\n",
        "\n",
        "Jab humara graph run ho raha hai, toh hum chahte hain ke messages humare messages state key mein append ho jayein.\n",
        "\n",
        "Hum yahan reducer functions ko istemal kar sakte hain iss masle ko hal karne ke liye.\n",
        "\n",
        "Reducers humein specify karne dete hain ke state update kis tarah perform honi chahiye.\n",
        "\n",
        "Agar koi reducer function specify na ho, toh assume kiya jata hai ke key ke updates usko overwrite kar denge jaisa ke hum pehle dekh chuke hain.\n",
        "\n",
        "Lekin, messages ko append karne ke liye, hum pre-built add_messages reducer ko istemal kar sakte hain.\n",
        "\n",
        "Yeh ensure karta hai ke koi bhi naya message purani messages list mein add ho jaye.\n",
        "\n",
        "Hum bas apni messages key ko add_messages reducer function ke saath metadata ke taur par annotate kar dete hain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b33eb72-3197-4870-b9a3-0da8056c40c5",
      "metadata": {
        "id": "6b33eb72-3197-4870-b9a3-0da8056c40c5"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "# Kisi variable ki type ko kuch extra maloomat ya context ke saath samjhana.\n",
        "# means user na age pochi but annotated ma age k sath month b btadi..3 year 5 mnth.\n",
        "from langgraph.graph.message import add_messages\n",
        "# new msg add kya\n",
        "class MessagesState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]\n",
        "    # ager annotated use krainga list ma  to add_msg k sath old msg b maintain rha\n",
        "    # ga or msg k variable ma store hojaiga"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3663e574-ba15-46be-a37c-48c8052d693b",
      "metadata": {
        "id": "3663e574-ba15-46be-a37c-48c8052d693b"
      },
      "source": [
        "Chunki graph state mein messages ki aik list rakhna bohat aam baat hai, isi liye LangGraph ne aik pre-built MessagesState diya hai!\n",
        "\n",
        "MessagesState kuch is tarah se define hota hai:\n",
        "\n",
        "Pehle se bane hue single messages key ke saath.\n",
        "Yeh aik AnyMessage objects ki list hoti hai.\n",
        "Yeh add_messages reducer ko istemal karta hai.\n",
        "Hum aam taur par MessagesState istemal karte hain kyunke yeh custom TypedDict define karne se kam verbose (mukhtasir) hai, jaisa ke hum ne upar dikhaya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ab516ee-eab1-4856-8210-99f1fe499672",
      "metadata": {
        "id": "9ab516ee-eab1-4856-8210-99f1fe499672"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import MessagesState\n",
        "\n",
        "class msg_conversational_State(MessagesState):\n",
        "    # Add any keys needed beyond messages, which is pre-built\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36b0fff7-60a2-4582-8f12-3a3ab6633d6c",
      "metadata": {
        "id": "36b0fff7-60a2-4582-8f12-3a3ab6633d6c"
      },
      "source": [
        "To go a bit deeper, we can see how the `add_messages` reducer works in isolation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23ffea76-16a5-4053-a1bc-91e0101d91dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23ffea76-16a5-4053-a1bc-91e0101d91dc",
        "outputId": "070afe98-e190-471a-e140-085e55e67f9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='Hello! How can I assist you?', additional_kwargs={}, response_metadata={}, name='Model', id='aa1c6c11-3153-4b73-a939-98eb86e5a1ff'),\n",
              " HumanMessage(content=\"I'm looking for information on marine biology.\", additional_kwargs={}, response_metadata={}, name='Lance', id='1d4d09fe-cf5a-4296-b17c-cf5da8f55305'),\n",
              " AIMessage(content='Sure, I can help with that. What specifically are you interested in?', additional_kwargs={}, response_metadata={}, name='Model', id='13138869-00d2-444f-b889-924a0e869655')]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "# Initial state\n",
        "initial_messages = [AIMessage(content=\"Hello! How can I assist you?\", name=\"Model\"),\n",
        "                    HumanMessage(content=\"I'm looking for information on marine biology.\", name=\"Lance\")\n",
        "                   ]\n",
        "                  # porana msg chk kya\n",
        "# New message to add\n",
        "new_message = AIMessage(content=\"Sure, I can help with that. What specifically are you interested in?\", name=\"Model\")\n",
        "#  new msg chk kya\n",
        "# Test\n",
        "add_messages(initial_messages , new_message)\n",
        "# dono ko print krwaya\n",
        "#  add_msg jo hama pre-build milrha tha wo thk kam krrha ha ya ni yha ye chk krrha ha.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "485adccc-f262-49dd-af4f-a30e9b6a48e2",
      "metadata": {
        "id": "485adccc-f262-49dd-af4f-a30e9b6a48e2"
      },
      "source": [
        "## Our graph\n",
        "\n",
        "Now, lets use ` msg_conversational_State` with a graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5306639-7e6a-44be-8471-8d2631701cfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "b5306639-7e6a-44be-8471-8d2631701cfb",
        "outputId": "0456758d-0d56-4691-db5c-99421836ff66"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "# Node\n",
        "def tool_calling_llm(state: msg_conversational_State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "#\n",
        "# Build graph\n",
        "builder: StateGraph = StateGraph( msg_conversational_State)\n",
        "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
        "builder.add_edge(START, \"tool_calling_llm\")\n",
        "builder.add_edge(\"tool_calling_llm\", END)\n",
        "graph: CompiledStateGraph  = builder.compile()\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8909771-7786-47d6-a53d-6bbc3b365737",
      "metadata": {
        "id": "e8909771-7786-47d6-a53d-6bbc3b365737"
      },
      "source": [
        "Agar hum input mein \"Hello!\" dein, toh LLM tool calls ke baghair jawab deta hai."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "983e2487-c0a5-40a2-afbc-aa53ff49fefc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "983e2487-c0a5-40a2-afbc-aa53ff49fefc",
        "outputId": "5210595d-4d1c-44d5-b406-40bedb90a877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hello im naveed!\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello Naveed! ðŸ‘‹  How can I help you today? ðŸ˜Š\n"
          ]
        }
      ],
      "source": [
        "graph.invoke({\"messages\": HumanMessage(content=\"Hello im naveed!\")})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3588688b-efd9-4dbc-abf2-7903e3ef89ba",
      "metadata": {
        "id": "3588688b-efd9-4dbc-abf2-7903e3ef89ba"
      },
      "source": [
        "The LLM chooses to use a tool when it determines that the input or task requires the functionality provided by that tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fe8b042-ecc8-426f-995e-cc1bbaf7cacc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fe8b042-ecc8-426f-995e-cc1bbaf7cacc",
        "outputId": "72b4ef38-17da-47a0-8482-b6a94871f3b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 4 and 2!\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (e297790b-2280-47a2-afbb-dfaeb4ef6fdc)\n",
            " Call ID: e297790b-2280-47a2-afbb-dfaeb4ef6fdc\n",
            "  Args:\n",
            "    b: 2.0\n",
            "    a: 4.0\n"
          ]
        }
      ],
      "source": [
        "messages = graph.invoke({\"messages\": HumanMessage(content=\"Multiply 4 and 2!\")})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "2-2_chains_reducers.learn_applied_generative_ai_fundamentals_03_langchain_ecosystem_langgraph_course_notebooks_module_1 ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}