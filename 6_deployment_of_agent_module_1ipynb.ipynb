{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NAVEED261/MY-AI-ASSISTANT/blob/main/6_deployment_of_agent_module_1ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c20242c4-0010-4065-89f6-0e0b16c7da6e",
      "metadata": {
        "id": "c20242c4-0010-4065-89f6-0e0b16c7da6e"
      },
      "source": [
        "# Deployment\n",
        "\n",
        "## Review\n",
        "\n",
        "We built up to an agent with memory:\n",
        "\n",
        "* `act` - let the model call specific tools\n",
        "* `observe` - pass the tool output back to the model\n",
        "* `reason` - let the model reason about the tool output to decide what to do next (e.g., call another tool or just respond directly)\n",
        "* `persist state` - use an in memory checkpointer to support long-running conversations with interruptions\n",
        "\n",
        "## Goals\n",
        "\n",
        "Now, we'll cover how to actually deploy our agent locally to Studio and to `LangGraph Cloud`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f348498b-f277-4514-b163-fe5ed9afe6fa",
      "metadata": {
        "id": "f348498b-f277-4514-b163-fe5ed9afe6fa"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph_sdk langchain_core"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4d0f4a7-82ee-4458-bd9a-e246ce2dc4ae",
      "metadata": {
        "id": "e4d0f4a7-82ee-4458-bd9a-e246ce2dc4ae"
      },
      "source": [
        "## Concepts\n",
        "\n",
        "There are a few central concepts to understand -\n",
        "\n",
        "`LangGraph` —\n",
        "- Python and JavaScript library\n",
        "- Allows creation of agent workflows\n",
        "\n",
        "[`LangGraph Server`](https://langchain-ai.github.io/langgraph/concepts/langgraph_server/) —\n",
        "- Bundles the graph code\n",
        "- Provides a task queue for managing asynchronous operations\n",
        "- Offers persistence for maintaining state across interactions\n",
        "- [Deployment options](https://langchain-ai.github.io/langgraph/concepts/deployment_options/) include LangGraph Platform and self-hosted deployment.\n",
        "\n",
        "`LangGraph Platform` --\n",
        "- Hosted service for the LangGraph API\n",
        "- Allows deployment of graphs from GitHub repositories\n",
        "- Also provides monitoring and tracing for deployed graphs\n",
        "- Accessible via a unique URL for each deployment\n",
        "\n",
        "`LangGraph Studio` --\n",
        "- Integrated Development Environment (IDE) for LangGraph applications\n",
        "- Uses the API as its back-end, allowing real-time testing and exploration of graphs\n",
        "- Can be run locally or with cloud-deployment\n",
        "\n",
        "`LangGraph SDK` --\n",
        "- Python and JS library for programmatically interacting with LangGraph graphs\n",
        "- Provides a consistent interface for working with graphs, whether served locally or in the cloud\n",
        "- Allows creation of clients, access to assistants, thread management, and execution of runs\n",
        "\n",
        "`RemoteGraph` --\n",
        "- The RemoteGraph class is a client implementation for calling remote APIs that implement the LangGraph Server API specification.\n",
        "\n",
        "\n",
        "\n",
        "## Testing Locally\n",
        "\n",
        "--\n",
        "\n",
        "**⚠️ DISCLAIMER**\n",
        "\n",
        "*Running Studio currently requires a Mac. If you are not using a Mac, then use langgraph cli for this step.*\n",
        "\n",
        "*Also, if you are running this notebook in Collab, run cloudflared or ngrok tunnel to access the API in Collab environment.*\n",
        "\n",
        "--\n",
        "\n",
        "We can easily connect with graphs that are served locally in LangGraph Studio!\n",
        "\n",
        "We do this via the `url` provided in the lower left corner of the Studio UI.\n",
        "\n",
        "![Screenshot 2024-08-23 at 1.17.05 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbad4f53080e6802cec34d_deployment%201.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fUS7IO0DT01K",
      "metadata": {
        "id": "fUS7IO0DT01K"
      },
      "source": [
        "### Local Setup Options\n",
        "\n",
        "1. [Clone Repo](https://github.com/panaversity/learn-agentic-ai-fundamentals)\n",
        "2. Open [module-1/studio](https://github.com/panaversity/learn-agentic-ai-fundamentals/tree/main/03_langchain_ecosystem/langgraph/course-notebooks/module-1/studio) locally in VS Code and add environment variables.\n",
        "\n",
        "Now use any of these options suitable for your machine.\n",
        "\n",
        "#### 1. [LangGraph Studio App](https://langchain-ai.github.io/langgraph/cloud/how-tos/test_local_deployment/) (Available only on Mac OS for now)\n",
        "\n",
        "#### 2. Run LangGraph Server Locally\n",
        "\n",
        "- Ensure you have `LANGSMITH_API_KEY` environment variable in .env file.\n",
        "\n",
        "```bash\n",
        "pip install langgraph-cli\n",
        "\n",
        "langgraph up\n",
        "```\n",
        "\n",
        "Once setup completes open in bowser:\n",
        "\n",
        "- Docs: http://127.0.0.1:8123/docs\n",
        "- [Access Studio](https://langchain-ai.github.io/langgraph/cloud/how-tos/test_local_deployment/#access-studio) in Browser: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8123\n",
        "\n",
        "\n",
        "Now continue the following locally or setup a tunnel to use Google Collab Environment:\n",
        "\n",
        "#### 3. Acces Local URL in Collab using NGROK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18b281d8-bd07-4721-922c-347838ceee6b",
      "metadata": {
        "id": "18b281d8-bd07-4721-922c-347838ceee6b"
      },
      "outputs": [],
      "source": [
        "from langgraph_sdk import get_client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c96f353-5dc3-41c8-a3e4-6bf07ca455f8",
      "metadata": {
        "id": "4c96f353-5dc3-41c8-a3e4-6bf07ca455f8"
      },
      "outputs": [],
      "source": [
        "# Replace this with the URL of your own deployed graph\n",
        "# URL = \"http://localhost:8123\" # Local Server URL\n",
        "URL = \"https://1da9-116-90-103-46.ngrok-free.app\"\n",
        "client = get_client(url=URL)\n",
        "\n",
        "# Search all hosted graphs\n",
        "# assistants = await client.assistants.search()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assistants = await client.assistants.search()"
      ],
      "metadata": {
        "id": "LCc5XA8jZQeM"
      },
      "id": "LCc5XA8jZQeM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a1352fa-68ad-4963-890e-c95d93570917",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a1352fa-68ad-4963-890e-c95d93570917",
        "outputId": "61eebbf9-9ad2-4b34-fb1a-860acd04d196"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca',\n",
              "  'graph_id': 'agent',\n",
              "  'created_at': '2024-11-18T05:14:53.266038+00:00',\n",
              "  'updated_at': '2024-11-18T05:14:53.266038+00:00',\n",
              "  'config': {},\n",
              "  'metadata': {'created_by': 'system'},\n",
              "  'version': 1,\n",
              "  'name': 'agent'},\n",
              " {'assistant_id': '228f9934-0cdd-5383-92c8-ee8422522cc2',\n",
              "  'graph_id': 'router',\n",
              "  'created_at': '2024-11-18T05:14:53.221352+00:00',\n",
              "  'updated_at': '2024-11-18T05:14:53.221352+00:00',\n",
              "  'config': {},\n",
              "  'metadata': {'created_by': 'system'},\n",
              "  'version': 1,\n",
              "  'name': 'router'},\n",
              " {'assistant_id': '28d99cab-ad6c-5342-aee5-400bd8dc9b8b',\n",
              "  'graph_id': 'simple_graph',\n",
              "  'created_at': '2024-11-18T05:14:51.682062+00:00',\n",
              "  'updated_at': '2024-11-18T05:14:51.682062+00:00',\n",
              "  'config': {},\n",
              "  'metadata': {'created_by': 'system'},\n",
              "  'version': 1,\n",
              "  'name': 'simple_graph'}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "assistants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba9c28a0-d712-496c-b191-7d620589ba33",
      "metadata": {
        "id": "ba9c28a0-d712-496c-b191-7d620589ba33"
      },
      "outputs": [],
      "source": [
        "# We create a thread for tracking the state of our run\n",
        "thread = await client.threads.create()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R3DnA7lhYM26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3DnA7lhYM26",
        "outputId": "8fd62f95-06e1-4116-94ea-e80b74563f4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'thread_id': 'e5b7ffca-25af-4c62-a6c6-e4e904b8b1d3',\n",
              " 'created_at': '2024-11-22T10:36:59.882489+00:00',\n",
              " 'updated_at': '2024-11-22T10:36:59.882489+00:00',\n",
              " 'metadata': {},\n",
              " 'status': 'idle',\n",
              " 'config': {},\n",
              " 'values': None}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "thread"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e7e4177-3644-43fa-a2f1-08f73292d1a6",
      "metadata": {
        "id": "2e7e4177-3644-43fa-a2f1-08f73292d1a6"
      },
      "source": [
        "Now, we can run our agent [with `client.runs.stream`](https://langchain-ai.github.io/langgraph/concepts/low_level/#stream-and-astream) with:\n",
        "\n",
        "* The `thread_id`\n",
        "* The `graph_id`\n",
        "* The `input`\n",
        "* The `stream_mode`\n",
        "\n",
        "We'll discuss streaming in depth in a future module.\n",
        "\n",
        "For now, just recognize that we are [streaming](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_values/) the full value of the state after each step of the graph with `stream_mode=\"values\"`.\n",
        "\n",
        "The state is captured in the `chunk.data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VRnnTpd4YKq-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRnnTpd4YKq-",
        "outputId": "41f349e1-5e1d-4a37-9616-176058764079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StreamPart(event='events', data={'event': 'on_chain_start', 'data': {'input': {'messages': [{'id': None, 'name': None, 'type': 'human', 'content': 'Hi', 'example': False, 'additional_kwargs': {}, 'response_metadata': {}}]}}, 'name': 'agent', 'tags': [], 'run_id': '1efa8bdb-8169-63b8-920c-392d93e91462', 'metadata': {'created_by': 'system', 'run_attempt': 1, 'langgraph_version': '0.2.53', 'langgraph_plan': 'developer', 'langgraph_host': 'self-hosted', 'run_id': '1efa8bdb-8169-63b8-920c-392d93e91462', 'user_id': '', 'graph_id': 'agent', 'thread_id': 'e5b7ffca-25af-4c62-a6c6-e4e904b8b1d3', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'x-forwarded-for': '34.80.234.115', 'x-forwarded-host': '1da9-116-90-103-46.ngrok-free.app', 'x-forwarded-proto': 'https'}, 'parent_ids': []})\n",
            "StreamPart(event='events', data={'event': 'on_chain_start', 'data': {'input': {'messages': [{'content': 'Hi', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'e8f55905-2c3c-4a68-94b4-778c95817292', 'example': False}]}}, 'name': 'assistant', 'tags': ['graph:step:1'], 'run_id': 'a7de3f60-25ae-47ba-adc7-e487daf20d0f', 'metadata': {'created_by': 'system', 'run_attempt': 1, 'langgraph_version': '0.2.53', 'langgraph_plan': 'developer', 'langgraph_host': 'self-hosted', 'run_id': '1efa8bdb-8169-63b8-920c-392d93e91462', 'user_id': '', 'graph_id': 'agent', 'thread_id': 'e5b7ffca-25af-4c62-a6c6-e4e904b8b1d3', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'x-forwarded-for': '34.80.234.115', 'x-forwarded-host': '1da9-116-90-103-46.ngrok-free.app', 'x-forwarded-proto': 'https', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ['__pregel_pull', 'assistant'], 'langgraph_checkpoint_ns': 'assistant:a30e0508-8da6-cbd4-ef6a-2ac844be6ed8'}, 'parent_ids': ['1efa8bdb-8169-63b8-920c-392d93e91462']})\n",
            "StreamPart(event='events', data={'event': 'on_chat_model_start', 'data': {'input': {'messages': [[{'content': 'You are a helpful assistant tasked with writing performing arithmetic on a set of inputs.', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'system', 'name': None, 'id': None}, {'content': 'Hi', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'e8f55905-2c3c-4a68-94b4-778c95817292', 'example': False}]]}}, 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'run_id': 'e118f295-2881-49e3-b0d3-c44c9c9c484d', 'metadata': {'created_by': 'system', 'run_attempt': 1, 'langgraph_version': '0.2.53', 'langgraph_plan': 'developer', 'langgraph_host': 'self-hosted', 'run_id': '1efa8bdb-8169-63b8-920c-392d93e91462', 'user_id': '', 'graph_id': 'agent', 'thread_id': 'e5b7ffca-25af-4c62-a6c6-e4e904b8b1d3', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'x-forwarded-for': '34.80.234.115', 'x-forwarded-host': '1da9-116-90-103-46.ngrok-free.app', 'x-forwarded-proto': 'https', 'langgraph_step': 1, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ['__pregel_pull', 'assistant'], 'langgraph_checkpoint_ns': 'assistant:a30e0508-8da6-cbd4-ef6a-2ac844be6ed8', 'checkpoint_ns': 'assistant:a30e0508-8da6-cbd4-ef6a-2ac844be6ed8', 'ls_provider': 'google_genai', 'ls_model_name': 'models/gemini-1.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1efa8bdb-8169-63b8-920c-392d93e91462', 'a7de3f60-25ae-47ba-adc7-e487daf20d0f']})\n",
            "StreamPart(event='error', data={'error': 'ChatGoogleGenerativeAIError', 'message': 'Invalid argument provided to Gemini: 400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\\ndomain: \"googleapis.com\"\\nmetadata {\\n  key: \"service\"\\n  value: \"generativelanguage.googleapis.com\"\\n}\\n, locale: \"en-US\"\\nmessage: \"API key expired. Please renew the API key.\"\\n]'})\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Input\n",
        "input = {\"messages\": [HumanMessage(content=\"Hi\")]}\n",
        "\n",
        "# Stream\n",
        "async for chunk in client.runs.stream(\n",
        "        thread['thread_id'],\n",
        "        \"agent\",\n",
        "        input=input,\n",
        "        stream_mode=\"events\",\n",
        "    ):\n",
        "    if chunk.data and chunk.event != \"metadata\":\n",
        "        print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tpZWZ6T_Zs0U",
      "metadata": {
        "id": "tpZWZ6T_Zs0U"
      },
      "source": [
        "#### **Note:** We will cover LangGraph Server (Agentic Infrastructure) in detail in the Module 6."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Input\n",
        "input = {\"messages\": [HumanMessage(content=\"Multiply 3 by 2.\")]}\n",
        "\n",
        "# Stream\n",
        "async for chunk in client.runs.stream(\n",
        "    thread['thread_id'],\n",
        "    \"agent\",\n",
        "    input=input,\n",
        "    stream_mode=\"events\",  # Change stream_mode to 'events'\n",
        "):\n",
        "    # Check if chunk has data and is not metadata and contains 'messages' key\n",
        "    if chunk.data and chunk.event != \"metadata\" and 'messages' in chunk.data:\n",
        "        # Access the last message in the 'messages' list\n",
        "        print(chunk.data['messages'][-1])\n",
        "    else:\n",
        "        # Handle other event types or log for debugging\n",
        "        print(f\"Received event: {chunk.event} with data: {chunk.data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GHZsaFebki5",
        "outputId": "32db34f0-10ce-41f1-876e-37db4960a22d"
      },
      "id": "_GHZsaFebki5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Received event: metadata with data: {'run_id': '1efa8bdb-b59a-67d0-a595-5bc04ddd17ba', 'attempt': 1}\n",
            "Received event: events with data: {'event': 'on_chain_start', 'data': {'input': {'messages': [{'id': None, 'name': None, 'type': 'human', 'content': 'Multiply 3 by 2.', 'example': False, 'additional_kwargs': {}, 'response_metadata': {}}]}}, 'name': 'agent', 'tags': [], 'run_id': '1efa8bdb-b59a-67d0-a595-5bc04ddd17ba', 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_attempt': 1, 'langgraph_version': '0.2.53', 'langgraph_plan': 'developer', 'langgraph_host': 'self-hosted', 'run_id': '1efa8bdb-b59a-67d0-a595-5bc04ddd17ba', 'user_id': '', 'thread_id': 'e5b7ffca-25af-4c62-a6c6-e4e904b8b1d3', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'x-forwarded-for': '34.80.234.115', 'x-forwarded-host': '1da9-116-90-103-46.ngrok-free.app', 'x-forwarded-proto': 'https'}, 'parent_ids': []}\n",
            "Received event: events with data: {'event': 'on_chain_start', 'data': {'input': {'messages': [{'content': 'Hi', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'e8f55905-2c3c-4a68-94b4-778c95817292', 'example': False}, {'content': 'Multiply 3 by 2.', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '2810fd85-86a4-489f-9e3e-a488dd706817', 'example': False}]}}, 'name': 'assistant', 'tags': ['graph:step:3'], 'run_id': '24a477d4-2a08-4feb-aff2-3736e66f734f', 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_attempt': 1, 'langgraph_version': '0.2.53', 'langgraph_plan': 'developer', 'langgraph_host': 'self-hosted', 'run_id': '1efa8bdb-b59a-67d0-a595-5bc04ddd17ba', 'user_id': '', 'thread_id': 'e5b7ffca-25af-4c62-a6c6-e4e904b8b1d3', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'x-forwarded-for': '34.80.234.115', 'x-forwarded-host': '1da9-116-90-103-46.ngrok-free.app', 'x-forwarded-proto': 'https', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ['__pregel_pull', 'assistant'], 'langgraph_checkpoint_ns': 'assistant:615f531c-8a40-80e5-8a0b-dcc5abae7756'}, 'parent_ids': ['1efa8bdb-b59a-67d0-a595-5bc04ddd17ba']}\n",
            "Received event: events with data: {'event': 'on_chat_model_start', 'data': {'input': {'messages': [[{'content': 'You are a helpful assistant tasked with writing performing arithmetic on a set of inputs.', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'system', 'name': None, 'id': None}, {'content': 'Hi', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'e8f55905-2c3c-4a68-94b4-778c95817292', 'example': False}, {'content': 'Multiply 3 by 2.', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '2810fd85-86a4-489f-9e3e-a488dd706817', 'example': False}]]}}, 'name': 'ChatGoogleGenerativeAI', 'tags': ['seq:step:1'], 'run_id': 'fda9c7b2-575d-4fbb-9ed5-a1929a74b252', 'metadata': {'graph_id': 'agent', 'created_by': 'system', 'run_attempt': 1, 'langgraph_version': '0.2.53', 'langgraph_plan': 'developer', 'langgraph_host': 'self-hosted', 'run_id': '1efa8bdb-b59a-67d0-a595-5bc04ddd17ba', 'user_id': '', 'thread_id': 'e5b7ffca-25af-4c62-a6c6-e4e904b8b1d3', 'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca', 'x-forwarded-for': '34.80.234.115', 'x-forwarded-host': '1da9-116-90-103-46.ngrok-free.app', 'x-forwarded-proto': 'https', 'langgraph_step': 3, 'langgraph_node': 'assistant', 'langgraph_triggers': ['start:assistant'], 'langgraph_path': ['__pregel_pull', 'assistant'], 'langgraph_checkpoint_ns': 'assistant:615f531c-8a40-80e5-8a0b-dcc5abae7756', 'checkpoint_ns': 'assistant:615f531c-8a40-80e5-8a0b-dcc5abae7756', 'ls_provider': 'google_genai', 'ls_model_name': 'models/gemini-1.5-flash', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['1efa8bdb-b59a-67d0-a595-5bc04ddd17ba', '24a477d4-2a08-4feb-aff2-3736e66f734f']}\n",
            "Received event: error with data: {'error': 'ChatGoogleGenerativeAIError', 'message': 'Invalid argument provided to Gemini: 400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\\ndomain: \"googleapis.com\"\\nmetadata {\\n  key: \"service\"\\n  value: \"generativelanguage.googleapis.com\"\\n}\\n, locale: \"en-US\"\\nmessage: \"API key expired. Please renew the API key.\"\\n]'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfa8b850-750c-4054-95e4-1c457a12ec8a",
      "metadata": {
        "id": "dfa8b850-750c-4054-95e4-1c457a12ec8a"
      },
      "source": [
        "## Testing with Cloud (Optional)\n",
        "\n",
        "We can deploy to Cloud via LangSmith, as outlined [here](https://langchain-ai.github.io/langgraph/cloud/quick_start/#deploy-from-github-with-langgraph-cloud).\n",
        "\n",
        "### Create a New Repository on GitHub\n",
        "\n",
        "* Go to your GitHub account\n",
        "* Click on the \"+\" icon in the upper-right corner and select `\"New repository\"`\n",
        "* Name your repository (e.g., `langchain-academy`)\n",
        "\n",
        "### Add Your GitHub Repository as a Remote\n",
        "\n",
        "* Go back to your terminal where you cloned `langchain-academy` at the start of this course\n",
        "* Add your newly created GitHub repository as a remote\n",
        "\n",
        "```\n",
        "git remote add origin https://github.com/your-username/your-repo-name.git\n",
        "```\n",
        "* Push to it\n",
        "```\n",
        "git push -u origin main\n",
        "```\n",
        "\n",
        "### Connect LangSmith to your GitHub Repository\n",
        "\n",
        "* Go to [LangSmith](hhttps://smith.langchain.com/)\n",
        "* Click on `deployments` tab on the left LangSmith panel\n",
        "* Add `+ New Deployment`\n",
        "* Then, select the Github repository (e.g., `langchain-academy`) that you just created for the course\n",
        "* Point the `LangGraph API config file` at one of the `studio` directories\n",
        "* For example, for module-1 use: `module-1/studio/langgraph.json`\n",
        "* Set your API keys (e.g., you can just copy from your `module-1/studio/.env` file)\n",
        "\n",
        "![Screenshot 2024-09-03 at 11.35.12 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbad4fd61c93d48e5d0f47_deployment2.png)\n",
        "\n",
        "### Work with your deployment\n",
        "\n",
        "We can then interact with our deployment a few different ways:\n",
        "\n",
        "* With the [SDK](https://langchain-ai.github.io/langgraph/cloud/quick_start/#use-with-the-sdk), as before.\n",
        "* With [LangGraph Studio](https://langchain-ai.github.io/langgraph/cloud/quick_start/#interact-with-your-deployment-via-langgraph-studio).\n",
        "\n",
        "![Screenshot 2024-08-23 at 10.59.36 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbad4fa159a09a51d601de_deployment3.png)\n",
        "\n",
        "To use the SDK here in the notebook, simply ensure that `LANGSMITH_API_KEY` is set!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "646ed351",
      "metadata": {
        "id": "646ed351"
      },
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"memory-enabled-agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97dda16c-c87f-4c03-b910-d647e83400b2",
      "metadata": {
        "id": "97dda16c-c87f-4c03-b910-d647e83400b2"
      },
      "outputs": [],
      "source": [
        "# Replace this with the URL of your deployed graph\n",
        "URL = \"https://1da9-116-90-103-46.ngrok-free.app\"\n",
        "client = get_client(url=URL)\n",
        "\n",
        "# Search all hosted graphs\n",
        "assistants = await client.assistants.search()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aefa37c0-92fe-4e80-9d5a-80a77b1e3dae",
      "metadata": {
        "id": "aefa37c0-92fe-4e80-9d5a-80a77b1e3dae"
      },
      "outputs": [],
      "source": [
        "# Select the agent\n",
        "agent = assistants[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b810376e-f20f-443a-b1ca-d6793f358f82",
      "metadata": {
        "id": "b810376e-f20f-443a-b1ca-d6793f358f82",
        "outputId": "cfc26445-a89a-43d1-befa-7f68f75f5011",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca',\n",
              " 'graph_id': 'agent',\n",
              " 'created_at': '2024-11-18T05:14:53.266038+00:00',\n",
              " 'updated_at': '2024-11-18T05:14:53.266038+00:00',\n",
              " 'config': {},\n",
              " 'metadata': {'created_by': 'system'},\n",
              " 'version': 1,\n",
              " 'name': 'agent'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "445cb34d-c3b8-4446-a7e3-5fe938abf99b",
      "metadata": {
        "id": "445cb34d-c3b8-4446-a7e3-5fe938abf99b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d84dc1-ede2-4a87-b0c9-195d5a47e703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'content': 'Multiply 3 by 2.', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '8187d11f-df41-4440-92db-8082a1c44d92', 'example': False}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# We create a thread for tracking the state of our run\n",
        "thread = await client.threads.create()\n",
        "\n",
        "# Input\n",
        "input = {\"messages\": [HumanMessage(content=\"Multiply 3 by 2.\")]}\n",
        "\n",
        "# Stream\n",
        "async for chunk in client.runs.stream(\n",
        "        thread['thread_id'],\n",
        "        \"agent\",\n",
        "        input=input,\n",
        "        stream_mode=\"values\",\n",
        "    ):\n",
        "    # Check if 'messages' key exists before accessing it\n",
        "    if chunk.data and chunk.event != \"metadata\" and 'messages' in chunk.data:\n",
        "        print(chunk.data['messages'][-1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "6_deployment of agent module_1ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}